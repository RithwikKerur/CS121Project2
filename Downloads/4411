https://www.ics.uci.edu/community/news/view_news?id=1817



In the News @ Donald Bren School of Information and Computer Sciences












Skip to main content





















HOME


ABOUT


About the School


Dean's Welcome


Facts and Figures


Donald Bren Hall


Visit the Bren School


Equity & Diversity
                            


Contact Us




DEPARTMENTS


Computer Science


Informatics


Statistics




RESEARCH


Research Areas


Research Centers


Research Highlights




STUDENTS


Graduate ▸


Orientation


Prospective Students


How to Apply


Programs of Study


Academic Year Plan


Course Updates


Forms And Policies


Admissions FAQ


Funding


Housing


Computing Support


Campus Resources


Visit the Bren School


Contact




Undergraduate ▸


Contact


Academic Advising


Course Updates


Academic Year Plan


Petitions


ICS Majors


ICS Minors


Policies: Academic Integrity


Policies: Academic Standing


Policies: Add, drop, & change options


Policies: Coursework outside UCI


Policies: Grade policy


Policies: Laptop & computer use


Policies: Withdrawal/Readmission


Policies: CS 199 / IN4MATX 199 Courses


Graduation


ICS Student Life


Upcoming Events


Computing Support




Scholarships & Fellowships


Student Jobs & Internships




PEOPLE


Faculty


Staff


Graduate Students




NEWS & EVENTS


Newsroom


Events


Social Media


Publications




GET INVOLVED


Make a Gift
                                



Leadership Council


Corporate Partners


Sponsor Projects


Product Development Competition


Alumni





























Bren school home / Community / News


In the News



July 21,  2020


Sameer Singh Wins Best Paper Award at ACL 2020 for Tool that Test NLP Models


In a recent tweet, Computer Science Professor Sameer Singh asked, “Are Natural Language Processing models as good as they seem on leaderboards?” He then provided the answer. “We know they’re not, but there isn’t a structured way to test them.” He went on to introduce CheckList, a task-agnostic methodology for testing NLP models that he developed in collaboration with Marco Tulio Ribeiro of Microsoft Research and Tongshuang Wu and Carlos Guestrin at the University of Washington. The team presented their paper, “Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,” at the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). Their work was not only well-received; it was the recipient of the Best Paper Award.

“As a community, we increasingly see NLP models that beat humans on accuracy on various datasets, yet we know that these models are not as good as humans for many of these tasks,” says Singh, explaining the motivation behind the work. “This made us think, what can we do about this mismatch in how we currently evaluate these models and what we think is their ‘true’ performance?”
Better Understanding NLP ModelsThe researchers addressed this mismatch using a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation. In addition, they developed a software tool that can quickly generate a large and diverse number of test cases. The resulting CheckList tool confirmed what the researchers suspected — that although certain commercial and research models could pass benchmark tests, further prodding revealed a variety of severe bugs and an inability to effectively handle basic linguistic phenomena.
For example, user studies discovered new and actionable bugs in cloud AI offerings from Amazon, Google and Microsoft. The models struggled to identify the sentiments of the following statements, showing unidentified failures when tested for temporal, negation and Semantic Role Labeling (SRL) capabilities, respectively:
“I used to hate this airline, although now I like it.”“I thought the plane would be awful, but it wasn’t.”“Do I think the pilot was fantastic? No.”
The models also had problems with an invariance (INV) test, changing the sentiment for the statement “@AmericanAir thank you we got on a different flight to Chicago,” when “Chicago” was replaced with “Dallas.”
This work illustrates the need for systematic testing in addition to standard evaluation. “These tasks may be considered ‘solved’ based on benchmark accuracy results,” note the authors, “but the tests highlight various areas of improvement — in particular, failure to demonstrate basic skills that are de facto needs for the task at hand (e.g. basic negation, agent/object distinction, etc.).”
The authors also evaluated Google’s BERT and Facebook AI’s RoBERTa algorithms, and, as pointed out in a recent Venture Beat article, found that BERT “exhibited gender bias in machine comprehension, overwhelmingly predicting men as doctors.” It also made “positive predictions about people who are straight or Asian and negative predictions when dealing with text about people who are atheist, Black, gay, or lesbian.” Identifying such algorithmic bias is critical to understanding how AI and machine learning can disrupt rather than perpetuate certain stereotypes.
Implementing CheckListDevelopment teams that participated in the user studies were very receptive to the work. One team responsible for a popular commercial sentiment analysis product said that CheckList helped them test capabilities that they hadn’t considered or that weren’t in the benchmarks. They were also able to more extensively test benchmarked capabilities and identify bugs that they could address in the next model iteration. The team was eager to incorporate CheckList into its development cycle.
“In our chats with NLP practitioners in the industry, it’s clear that current practices in ‘testing’ NLP systems, or machine learning in general, are quite heuristic and ad-hoc,” says Singh. “Checklist provides a structure and the tooling to make it easier to think about testing, and provides a lot of evidence that thinking more about testing can lead to really important insights and discovery of bugs.”
All of the tests presented in the paper are part of CheckList’s open source release and can be easily incorporated to complement existing benchmarks. An online video walks viewers through the findings. Furthermore, CheckList can be used to collectively create more exhaustive test suites for a variety of tasks to help developers gain a more accurate understanding of the true capabilities — and flaws — of their systems.
– Shani Murray

Sameer Singh Wins Best Paper Award at ACL 2020 for Tool that Test NLP Models
< Previous
 Informatics Professors Ahmed, Branham Receive Teach Access Curriculum Development Awards
            
Next >
 GeekED: Re-storied: Re-imagining Creative Privilege (CCEL)
            







More news and events

News Stories
Video highlights
Annual reports
Calendar of events


Media Inquiries Media interested in interviewing ICS faculty, students or alumni should contact Matt Miller at (949) 824-1562 or matt.miller@uci.edu








Tweets by UCIbrenICS 

View Social Media












                        Donald Bren School of Information and Computer Sciences
                		University of California, Irvine
    					6210 Donald Bren Hall
    					Irvine, CA 92697-3425
                    



  
  
  
  
  
 



UCI Homepage
UCI Directory
Faculty & Staff
Employment
ICS Intranet
Research Development
Kay Family Foundation Lab
Facilities




News
Events
Map & Directions
Accessibility Statement
Safety Procedures
Privacy Policy
Coronavirus Resources




Apply
Contact
Gift 




©  UCI Donald Bren School of Information & Computer Sciences






➤











