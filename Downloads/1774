http://sli.ics.uci.edu/Classes/2015W-273a



SLI | Classes / CS273a: Introduction to Machine Learning 


 












(?)





Classes
Group
Research
Publications
Code






login




Classes /
CS273a: Introduction to Machine Learning



Assignments and Exams:

HW1Code01/13/15Soln 
HW2Code01/20/15Soln 
HW3Code01/27/15Soln 
MidtermIn-class2/10/15  
HW4Code02/24/15Soln 
HW5Code03/10/15Soln 
Project 3/20/15 
FinalTue 4:00-6:00pm3/17/15  

Lecture: Tues/Thurs 3:30pm-5:00pm, ICS 174
Instructor: Prof. Alex Ihler (ihler@ics.uci.edu), Office Bren Hall 4066
Office Hours: Mon 10:30-12:00pm, Bren Hall 4066, or by appointment
Course Notes in development
Also, a possibly helpful LaTeX template I use for homeworks and solutions.
(Or, this link has another nice way to include Matlab code in LaTeX.)

Introduction to machine learning and data mining
How can a machine learn from experience, to become better at a given task?  How can we automatically extract knowledge or make sense of massive quantities of data?  These are the fundamental questions of machine learning.  Machine learning and data mining algorithms use techniques from statistics, optimization, and computer science to create automated systems which can sift through large volumes of data at high speed to make predictions or decisions without human intervention.

Machine learning as a field is now incredibly pervasive, with applications from the web (search, advertisements, and suggestions) to national security, from analyzing biochemical interactions to traffic and emissions to astrophysics.  Perhaps most famously, the $1M Netflix prize stirred up interest in learning algorithms in professionals, students, and hobbyists alike.

This class will familiarize you with a broad cross-section of models and algorithms for machine learning, and prepare you for research or industry application of machine learning techniques. 

Background
We will assume basic familiarity with the concepts of probability and linear algebra.  Some programming will be required; we will primarily use Matlab, but no prior experience with Matlab will be assumed.  (Most or all code should be Octave compatible, so you may use Octave if you prefer.)

Textbook and Reading
There is no required textbook for the class.  However, useful books on the subject for supplementary reading include
Murphy's "Machine Learning: A Probabilistic Perspective", Duda, Hart & Stork, "Pattern Classification", and Hastie, Tibshirani, and Friedman, "The Elements of Statistical Learning".

Piazza
I use Piazza to manage student discussions and questions.  Our class link is:
http://piazza.com/uci/winter2015/cs273a.

Matlab
Often we will write code for the course using the Matlab environment.  Matlab is accessible through NACS computers at several campus locations (e.g., 
MSTB-A, 
MSTB-B,
and the ICS lab), and student licenses are fairly inexpensive ($100) and even free for UCI students on personal machines; see here for instructions.  If you use Octave, please be careful to use Matlab-compatible syntax (not Octave extensions), since otherwise I or the grader may be unable to interpret your code.
For Octave, I suggest the SourceForge Octave binaries, e.g., Windows and Mac.

If you are not familiar with Matlab, there are a number of tutorials on the web:
Union College / Cyclismo.Org tutorial, more complete
TutorialsPoint
A basic PDF guide by David Houcque
A far more detailed PDF guide by Ed Overman
You may want to start with one of the very short tutorials, then use the longer ones as a reference during the rest of the term.

We will also use Matlab classes, including several classes that I've written for the course, 
to test and evaluate various classifiers.  Here are some Notes on Matlab Object-Oriented Programming, using our k-nearest neighbor class as a working example.


Syllabus (subject to change)
SlidesVideosTopics
PDF1 , 2 , 3 , 4Introduction
PDF1 , 2Nearest neighbor methods
PDF1 , 2Bayes classifiers, naive Bayes
PDF1 , 2 , 3 , 4 , 5 , 6Linear regression
PDF1 , 2Linear classifiers; perceptrons & logistic regression
PDF1VC dimension, shattering, and complexity
PDF1 , 2Neural networks (multi-layer perceptrons) and deep belief nets
PDF1 , 2 , 3Support vector machines; kernel methods
PDF1 , 2Decision trees for classification & regression
PDF1, 2, 3, 4Ensembles; bagging, gradient boosting, adaboost
PDF1 , 2 , 3 , 4Unsupervised learning: clustering methods
PDF1, 2Dimensionality reduction: (Multivariate Gaussians); PCA/SVD, latent space representations
PDF Recommender Systems and Collaborative Filtering
PDF Time series, Markov models
PDF Markov Decision Processes (slides from Andrew Moore)


Course Project
See Project Description pdf on the upper-right of this page (with homework & exams)







 Last modified March 11, 2015, at 11:50 AM

Bren School of Information and Computer Science
University of California, Irvine





